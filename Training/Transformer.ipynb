{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merge_train.csv',parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['weekdays'] = df['date'].dt.dayofweek  # 1 for weekend, 0 for weekday\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['minute'] = df['date'].dt.minute\n",
    "# df['EMA_4h'] = df['Price'].ewm(span=48, adjust=False).mean()\n",
    "\n",
    "\n",
    "df['SMA_4h'] = df['Price'].shift(1).rolling(window=48).mean()\n",
    "# df['EMA_1d'] = df['Price'].ewm(span=288, adjust=False).mean()\n",
    "df['Forecast_difference_0.5 hour future'] = df['Forecast_Roof_0.5 hour future'] - df['Forecast_Demand_0.5 hour future']\n",
    "df[\"Forecast_ratio_0.5 hour future\"] = df['Forecast_Roof_0.5 hour future'] / df['Forecast_Demand_0.5 hour future']\n",
    "df.set_index('date', inplace=True)\n",
    "df['Demand_lag1'] = df['Demand'].shift(1)  \n",
    "df['Actual_Demand_OPERATIONAL_DEMAND_lag1'] = df['Actual_Demand_OPERATIONAL_DEMAND'].shift(1)  \n",
    "\n",
    "\n",
    "df = df.drop(['Demand', 'Actual_Demand_OPERATIONAL_DEMAND'], axis=1)\n",
    "\n",
    "# 去除第一行的 NaN 值（因為移動了一步，第一行的滯後特徵會是空的）\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.drop(columns=['Region', 'Actual_Roof_OPOWER', 'Actual_Roof_LASTCHANGED'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, time\n",
    "\n",
    "def create_qld_holidays_list():\n",
    "    holidays = [\n",
    "        # 2023\n",
    "        \"2023-01-01\", \"2023-01-02\", \"2023-01-26\", \"2023-04-07\", \"2023-04-08\", \"2023-04-09\", \"2023-04-10\",\n",
    "        \"2023-04-25\", \"2023-05-01\", \"2023-08-16\", \"2023-10-02\", \"2023-12-25\", \"2023-12-26\",\n",
    "        # 2024\n",
    "        \"2024-01-01\", \"2024-01-26\", \"2024-03-29\", \"2024-03-30\", \"2024-03-31\", \"2024-04-01\",\n",
    "        \"2024-04-25\", \"2024-05-06\", \"2024-08-14\", \"2024-10-07\", \"2024-12-25\", \"2024-12-26\",\n",
    "        # 2025\n",
    "        \"2025-01-01\", \"2025-01-27\", \"2025-04-18\", \"2025-04-19\", \"2025-04-20\", \"2025-04-21\",\n",
    "        \"2025-04-25\", \"2025-05-05\", \"2025-08-13\", \"2025-10-06\", \"2025-12-25\", \"2025-12-26\"\n",
    "    ]\n",
    "    return pd.to_datetime(holidays)\n",
    "\n",
    "def add_qld_holidays(df):\n",
    "    holidays = create_qld_holidays_list()\n",
    "    \n",
    "   \n",
    "    df['is_holiday'] = 0\n",
    "    \n",
    "    df.loc[df.index.isin(holidays), 'is_holiday'] = 1\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_qld_holidays(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Price', axis=1)\n",
    "y = df['Price']\n",
    "\n",
    "train_end_date = pd.to_datetime(\"2024-04-30 23:59:59\")\n",
    "val_end_date = pd.to_datetime(\"2024-05-31 23:59:59\")\n",
    "test_start_date = pd.to_datetime(\"2024-06-01 00:00:00\")\n",
    "\n",
    "\n",
    "X_train = X[X.index <= train_end_date]\n",
    "X_val = X[(X.index > train_end_date) & (X.index <= val_end_date)]\n",
    "X_test = X[X.index >= test_start_date]\n",
    "\n",
    "y_train = y[y.index <= train_end_date]\n",
    "y_val = y[(y.index > train_end_date) & (y.index <= val_end_date)]\n",
    "y_test = y[y.index >= test_start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=20000):\n",
    "        super(PositionalEncoding, self).__init__()       \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransAm(nn.Module):\n",
    "    def __init__(self, feature_size=50, num_layers=1, dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        batch_size, seq_length, _ = src.size()\n",
    "        if self.src_mask is None or self.src_mask.size(0) != seq_length:\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(seq_length).to(device)  # 使用序列長度生成掩碼\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, y, seq_length):\n",
    "    X_seq, y_seq, dates_seq = [], [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        y_seq.append(y[i:i+seq_length])\n",
    "        dates_seq.append(X.index[i+seq_length])  \n",
    "    return np.array(X_seq), np.array(y_seq), dates_seq\n",
    "\n",
    "\n",
    "seq_length = 10\n",
    "\n",
    "\n",
    "X_train_seq, y_train_seq,train_date_seq = prepare_data(X_train, y_train, seq_length)\n",
    "X_val_seq, y_val_seq,val_date_seq = prepare_data(X_val, y_val, seq_length)\n",
    "X_test_seq, y_test_seq,test_date_seq = prepare_data(X_test, y_test, seq_length)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val_seq, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_seq, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 超參數\n",
    "input_size = X_train_tensor.shape[2]  \n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 0.002\n",
    "num_heads = 8\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 區塊 6：初始化模型\n",
    "\n",
    "\n",
    "model = TransAm(feature_size=input_size).to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 51.271971, Val Loss: 47.234866\n",
      "Epoch 2/50, Train Loss: 45.381734, Val Loss: 43.671530\n",
      "Epoch 3/50, Train Loss: 44.606228, Val Loss: 42.376159\n",
      "Epoch 4/50, Train Loss: 44.213816, Val Loss: 41.131616\n",
      "Epoch 5/50, Train Loss: 43.913776, Val Loss: 40.685028\n",
      "Epoch 6/50, Train Loss: 43.706592, Val Loss: 40.036596\n",
      "Epoch 7/50, Train Loss: 43.522112, Val Loss: 39.734831\n",
      "Epoch 8/50, Train Loss: 43.285966, Val Loss: 39.204464\n",
      "Epoch 9/50, Train Loss: 43.120352, Val Loss: 38.775356\n",
      "Epoch 10/50, Train Loss: 42.937210, Val Loss: 38.215156\n",
      "Epoch 11/50, Train Loss: 42.810482, Val Loss: 38.172967\n",
      "Epoch 12/50, Train Loss: 42.689152, Val Loss: 38.067807\n",
      "Epoch 13/50, Train Loss: 42.612187, Val Loss: 37.568878\n",
      "Epoch 14/50, Train Loss: 42.507867, Val Loss: 37.446195\n",
      "Epoch 15/50, Train Loss: 42.418919, Val Loss: 37.226384\n",
      "Epoch 16/50, Train Loss: 42.338186, Val Loss: 36.942995\n",
      "Epoch 17/50, Train Loss: 42.234968, Val Loss: 36.898240\n",
      "Epoch 18/50, Train Loss: 42.128993, Val Loss: 36.713292\n",
      "Epoch 19/50, Train Loss: 42.049747, Val Loss: 36.522177\n",
      "Epoch 20/50, Train Loss: 41.937046, Val Loss: 36.384328\n",
      "Epoch 21/50, Train Loss: 41.892626, Val Loss: 36.224564\n",
      "Epoch 22/50, Train Loss: 41.792085, Val Loss: 36.120006\n",
      "Epoch 23/50, Train Loss: 41.684780, Val Loss: 35.999209\n",
      "Epoch 24/50, Train Loss: 41.614162, Val Loss: 36.112845\n",
      "Epoch 25/50, Train Loss: 41.537584, Val Loss: 36.085327\n",
      "Epoch 26/50, Train Loss: 41.461013, Val Loss: 35.940832\n",
      "Epoch 27/50, Train Loss: 41.436127, Val Loss: 35.806306\n",
      "Epoch 28/50, Train Loss: 41.336841, Val Loss: 35.855017\n",
      "Epoch 29/50, Train Loss: 41.292797, Val Loss: 35.577932\n",
      "Epoch 30/50, Train Loss: 41.223257, Val Loss: 35.568496\n",
      "Epoch 31/50, Train Loss: 41.171364, Val Loss: 35.474701\n",
      "Epoch 32/50, Train Loss: 41.134239, Val Loss: 35.369941\n",
      "Epoch 33/50, Train Loss: 41.048886, Val Loss: 35.376780\n",
      "Epoch 34/50, Train Loss: 40.973074, Val Loss: 35.272928\n",
      "Epoch 35/50, Train Loss: 40.918296, Val Loss: 35.153266\n",
      "Epoch 36/50, Train Loss: 40.901229, Val Loss: 35.283275\n",
      "Epoch 37/50, Train Loss: 40.866197, Val Loss: 35.382794\n",
      "Epoch 38/50, Train Loss: 40.840621, Val Loss: 35.194309\n",
      "Epoch 39/50, Train Loss: 40.775267, Val Loss: 35.353534\n",
      "Epoch 40/50, Train Loss: 40.733707, Val Loss: 35.188215\n",
      "Epoch 41/50, Train Loss: 40.673818, Val Loss: 35.044804\n",
      "Epoch 42/50, Train Loss: 40.666574, Val Loss: 35.086842\n",
      "Epoch 43/50, Train Loss: 40.613276, Val Loss: 35.134925\n",
      "Epoch 44/50, Train Loss: 40.575390, Val Loss: 35.240070\n",
      "Epoch 45/50, Train Loss: 40.554500, Val Loss: 35.134609\n",
      "Epoch 46/50, Train Loss: 40.529628, Val Loss: 35.076849\n",
      "Epoch 47/50, Train Loss: 40.492376, Val Loss: 35.228110\n",
      "Epoch 48/50, Train Loss: 40.282568, Val Loss: 35.278368\n",
      "Epoch 49/50, Train Loss: 40.146023, Val Loss: 35.354719\n",
      "Epoch 50/50, Train Loss: 40.084542, Val Loss: 35.245670\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        \n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch_X.size(0)\n",
    "        \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            val_loss += loss.item() * batch_X.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'trans_am_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'trans_am_model.pth')\n",
    "\n",
    "print(\"Model saved as 'trans_am_model.pth'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DayLocator, HourLocator, DateFormatter\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 假设 results_df 已经包含 'Actual' 和 'Predicted' 的数据，以及相应的时间索引\n",
    "# 例如:\n",
    "# results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}, index=y_test.index)\n",
    "\n",
    "def plot_three_day_comparison(data, start_date, end_date):\n",
    "    \"\"\"绘制实际值与预测值的对比图，按天进行比较\"\"\"\n",
    "    period_data = data.loc[start_date:end_date]\n",
    "    \n",
    "    # 创建图形对象和子图\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    \n",
    "    # 绘制实际值和预测值\n",
    "    ax.plot(period_data.index, period_data['Actual'], label='Actual', marker='o', markersize=4)\n",
    "    ax.plot(period_data.index, period_data['Predicted'], label='Predicted', marker='o', markersize=4)\n",
    "    \n",
    "    # 设置图形标题和轴标签\n",
    "    ax.set_title(f'Actual vs Predicted Prices ({start_date.strftime(\"%Y-%m-%d %H:%M\")} to {end_date.strftime(\"%Y-%m-%d %H:%M\")})')\n",
    "    ax.set_xlabel('Date and Time')\n",
    "    ax.set_ylabel('Price')\n",
    "    ax.legend()\n",
    "\n",
    "    # 设置日期格式和时间间隔\n",
    "    ax.xaxis.set_major_locator(HourLocator(interval=6))  # 主要坐标点每隔6小时\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%m-%d %H:%M'))  # 格式化坐标点日期显示\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 计算评估指标\n",
    "    mse = mean_squared_error(period_data['Actual'], period_data['Predicted'])\n",
    "    mae = mean_absolute_error(period_data['Actual'], period_data['Predicted'])\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(period_data['Actual'], period_data['Predicted'])\n",
    "    \n",
    "    # 在图形上显示评估指标\n",
    "    metrics_text = f'MSE: {mse:.4f}\\nMAE: {mae:.4f}\\nRMSE: {rmse:.4f}\\nR²: {r2:.4f}'\n",
    "    plt.text(0.02, 0.98, metrics_text, transform=ax.transAxes, verticalalignment='top', \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 显示图形\n",
    "    plt.show()\n",
    "\n",
    "# 设置起始日期和结束日期\n",
    "start_date = results_df.index.min()\n",
    "end_date = results_df.index.max()\n",
    "\n",
    "# 循环按每三天绘制图形\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    period_end = min(current_date + pd.Timedelta(days=3), end_date)\n",
    "    plot_three_day_comparison(results_df, current_date, period_end)\n",
    "    current_date += pd.Timedelta(days=3)\n",
    "\n",
    "# 计算总体性能指标\n",
    "mse = mean_squared_error(results_df['Actual'], results_df['Predicted'])\n",
    "mae = mean_absolute_error(results_df['Actual'], results_df['Predicted'])\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(results_df['Actual'], results_df['Predicted'])\n",
    "\n",
    "# 输出总体性能指标\n",
    "print(f\"Overall Performance Metrics:\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"R-squared Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18411, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (18411, 10) instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual\u001b[39m\u001b[38;5;124m'\u001b[39m: y_test_actual,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions\n\u001b[0;32m      4\u001b[0m }, index\u001b[38;5;241m=\u001b[39my_test\u001b[38;5;241m.\u001b[39mindex[seq_length:])\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:119\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     arrays, refs \u001b[38;5;241m=\u001b[39m _homogenize(arrays, index, dtype)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    127\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:629\u001b[0m, in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    626\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(val)\n\u001b[0;32m    627\u001b[0m     val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m--> 629\u001b[0m val \u001b[38;5;241m=\u001b[39m sanitize_array(val, index, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    630\u001b[0m com\u001b[38;5;241m.\u001b[39mrequire_length_match(val, index)\n\u001b[0;32m    631\u001b[0m refs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\pandas\\core\\construction.py:659\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    656\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n\u001b[0;32m    657\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m maybe_infer_to_datetimelike(subarr)\n\u001b[1;32m--> 659\u001b[0m subarr \u001b[38;5;241m=\u001b[39m _sanitize_ndim(subarr, data, dtype, index, allow_2d\u001b[38;5;241m=\u001b[39mallow_2d)\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[0;32m    663\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mdtype, dtype)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\pandas\\core\\construction.py:718\u001b[0m, in \u001b[0;36m_sanitize_ndim\u001b[1;34m(result, data, dtype, index, allow_2d)\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allow_2d:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData must be 1-dimensional, got ndarray of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    720\u001b[0m     )\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001b[39;00m\n\u001b[0;32m    724\u001b[0m     result \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: Data must be 1-dimensional, got ndarray of shape (18411, 10) instead"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test_actual,\n",
    "    'Predicted': predictions\n",
    "}, index=y_test.index[seq_length:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单元格 2：定义绘图函数\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DayLocator, HourLocator, DateFormatter\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def plot_three_day_comparison(data, start_date, end_date):\n",
    "    period_data = data.loc[start_date:end_date]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(period_data.index, period_data['Actual'], label='Actual price', marker='o', markersize=4)\n",
    "    ax.plot(period_data.index, period_data['Predicted'], label='predict price', marker='o', markersize=4)\n",
    "    \n",
    "    ax.set_title(f'（{start_date.strftime(\"%Y-%m-%d %H:%M\")} to {end_date.strftime(\"%Y-%m-%d %H:%M\")}）')\n",
    "    ax.set_xlabel('date')\n",
    "    ax.set_ylabel('Price')\n",
    "    ax.legend()\n",
    "    \n",
    "\n",
    "    ax.xaxis.set_major_locator(HourLocator(interval=6))\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%m-%d %H:%M'))\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    mse = mean_squared_error(period_data['Actual'], period_data['Predicted'])\n",
    "    mae = mean_absolute_error(period_data['Actual'], period_data['Predicted'])\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(period_data['Actual'], period_data['Predicted'])\n",
    "    \n",
    "    metrics_text = f'MSE: {mse:.4f}\\nMAE: {mae:.4f}\\nRMSE: {rmse:.4f}\\n$R^2$: {r2:.4f}'\n",
    "    plt.text(0.02, 0.98, metrics_text, transform=ax.transAxes, verticalalignment='top', \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "start_date = results_df.index.min()\n",
    "end_date = results_df.index.max()\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    period_end = min(current_date + pd.Timedelta(days=3), end_date)\n",
    "    plot_three_day_comparison(results_df, current_date, period_end)\n",
    "    current_date += pd.Timedelta(days=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def is_prediction_successful(actual, predicted, tolerance=0.1):\n",
    "    return abs(predicted - actual) <= abs(actual * tolerance)\n",
    "\n",
    "\n",
    "results_df['is_successful'] = results_df.apply(lambda row: is_prediction_successful(row['Actual'], row['Predicted']), axis=1)\n",
    "\n",
    "\n",
    "results_df['hour'] = results_df.index.hour\n",
    "hourly_success_rate = results_df.groupby('hour')['is_successful'].mean().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "hourly_prediction_count = results_df.groupby('hour').size()\n",
    "\n",
    "\n",
    "print(\"Hourly Success Rate (sorted from highest to lowest):\")\n",
    "print(hourly_success_rate)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "hourly_success_rate.plot(kind='bar')\n",
    "plt.title('Prediction Success Rate by Hour (10% Tolerance)')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "hourly_prediction_count.plot(kind='bar')\n",
    "plt.title('Number of Predictions by Hour')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Predictions')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "top_5_hours = hourly_success_rate.nlargest(5)\n",
    "bottom_5_hours = hourly_success_rate.nsmallest(5)\n",
    "\n",
    "print(\"\\nTop 5 hours with highest success rate:\")\n",
    "print(top_5_hours)\n",
    "\n",
    "print(\"\\nBottom 5 hours with lowest success rate:\")\n",
    "print(bottom_5_hours)\n",
    "\n",
    "# 計算總體成功率\n",
    "overall_success_rate = results_df['is_successful'].mean()\n",
    "print(f\"\\nOverall Success Rate: {overall_success_rate:.2%}\")\n",
    "\n",
    "\n",
    "# fig, ax1 = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# ax1.bar(hourly_success_rate.index, hourly_success_rate.values, alpha=0.7, color='b', label='Success Rate')\n",
    "# ax1.set_xlabel('Hour of Day')\n",
    "# ax1.set_ylabel('Success Rate', color='b')\n",
    "# ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(hourly_prediction_count.index, hourly_prediction_count.values, color='r', label='Prediction Count')\n",
    "# ax2.set_ylabel('Number of Predictions', color='r')\n",
    "# ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "# plt.title('Prediction Success Rate and Count by Hour')\n",
    "# fig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TimeSeries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
